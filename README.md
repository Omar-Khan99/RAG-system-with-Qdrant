# ğŸ§  RAG System with Qdrant & Ollama

A production-ready **Retrieval-Augmented Generation (RAG)** system built with:
- **Qdrant** â€” high-performance vector database  
- **Ollama** â€” local LLM & embedding inference  
- **FastAPI** â€” modern REST API backend  
- **Docker Compose** â€” fully containerized, reproducible deployment

âœ… **Fully working out-of-the-box** â€” models & data persist between restarts.

---

## ğŸš€ Features

- âœ… **True offline RAG**: Runs 100% locally using Ollama (no cloud APIs)
- âœ… **Persistent storage**: Qdrant data + Ollama models survive restarts
- âœ… **Semantic search + LLM generation** in one pipeline
- âœ… **Hot-reload** for development (`--reload`)
- âœ… **Docker-first design** â€” no Python/env setup needed on host
- âœ… Optimized with `uv` for ultra-fast dependency resolution & locking

---

## âš™ï¸ Prerequisites

Install these **once** on your machine:

| Tool | Purpose | Install Guide |
|------|---------|---------------|
| ğŸ³ **Docker** | Container runtime | [docker.com](https://www.docker.com/products/docker-desktop) |
| ğŸ³ **Docker Compose** | Multi-container orchestration | Included in Docker Desktop (v2.20+) |

> âœ… No Python, `uv`, or Ollama installation needed on your host OS â€” everything runs in containers.

---

## â–¶ï¸ Quick Start (Recommended)

### 1. Clone & enter the project
``` bash
git clone https://github.com/Omar-Khan99/RAG-system-with-Qdrant.git
cd RAG-system-with-Qdrant
```

### 2. Start the system (first run = downloads models)
``` bash
docker compose up --build
```
â³ First run takes time (downloads `all-minilm:l6-v2` + `llama3:8b` ~5GB), but subsequent runs are instant.

### 3. Use the API
Once you see:

```
ollama-1  | âœ… Models ready. Keeping server running...
app-1     | INFO:     Uvicorn running on http://0.0.0.0:8000
```

Open in your browser:

ğŸ”¹ [Swagger UI (interactive docs)](http://localhost:8000/docs) 

 ğŸ”¹ [Qdrant Dashboard](http://localhost:6333/dashboard)

---

## ğŸ” Daily Workflow (Fast Restart)
After first setup, use these for quick stop/start without re-downloading:

| COMMAND | DESCRIPTION |
|------|--------------|
| `docker compose down` | âœ… **Stop services** (keeps data & models!) |
| `docker compose up` | âœ… **Restart in <10 seconds** (no rebuild, no redownload) |
`docker compose up -d` | Run in background (detached mode) |
`docker compose logs -f app` | Stream app logs only

> "ğŸ’¡ Never use -v with down unless you want to wipe all data/models. "

## ğŸ§ª API Endpoints

| ENDPOINT | METHOD | DESCRIPTION |
|------|---------|---------------|
| `/docs`| GET | Interactive API documentation (Swagger UI)
 |
| `/api/v1/upload-file/` | POST | Upload PDF/text and index into Qdrant
 | `/ask` | POST |  Ask questions: `?query=What is Bayanat?&limit=5` |
  `/api/v1/files/` | GET | List uploaded documents


## ğŸ§  How It Works

1. **Document Upload:** PDF/text â†’ chunked â†’ embedded with `all-minilm:l6-v2` â†’ stored in **Qdrant**

2. **Query:** User question â†’ embedded â†’ top-k similar chunks retrieved from Qdrant

3. **Generation:** Chunks + question â†’ sent to `llama3:8b` via **Ollama** â†’ final answer

> "All models run locally on your machine â€” no external API calls. "